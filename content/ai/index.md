title=AI
category=ai
type=categorypage
summary=A collection of links around artificial intelligence (and all the different permutations of that phrase).
tags=ai
~~~~~~

## Uncategorized (yet)

## General

- [The 10+1 Commandments of Human AI Co-Existence](https://10plus1.ai/): "The 10+1 is a standard. Not everyone will adopt it - but those who do will lead the next era. If you're a policymaker, don't wait for regulation to catch up. If you're a CEO, don't wait for PR fallout to act with integrity. If you're building AI, don't just ask if it works - ask if it's worthy. We don't need more frameworks. We need a philosophical backbone. This is it."
- [AI Defeats the Purpose of a Humanities Education](https://www.thecrimson.com/article/2025/9/9/chiocco-farrell-harvard-ai/) (The Harvard Crimson)
- ["The Open-Source Toolkit for Building AI agents"](https://www.aitidbits.ai/p/open-source-agents)
- [Is Complexity an Illusion?](https://arxiv.org/pdf/2404.07227): "Simplicity is held by many to be the key to general intelligence. Simpler models tend to “generalise”, identifying the cause or generator of data with greater sample efficiency. The implications of the correlation between simplicity and generalisation extend far beyond computer science, addressing questions of physics and even biology. Yet simplicity is a property of form, while generalisation is of function. In interactive settings, any correlation between the two depends on interpretation. In theory there could be no correlation and yet in practice, there is. Previous theoretical work showed generalisation to be a consequence of “weak” constraints implied by function, not form. Experiments demonstrated choosing weak constraints over simple forms yielded a 110 − 500% improvement in generalisation rate. Here we show that all constraints can take equally simple forms, regardless of weakness. However if forms are spatially extended, then function is represented using a finite subset of forms. If function is represented using a finite subset of forms, then we can force a correlation between simplicity and generalisation by making weak constraints take simple forms. If function is determined by a goal directed process that favours versatility (e.g. natural selection), then efficiency demands weak constraints take simple forms. Complexity has no causal influence on generalisation, but appears to due to confounding."
- Rob Conery's posts exploring using AI in a dev setting:

    - ["Using Gemini CLI to Learn Something New"](https://thenext.dev/articles/all/using-gemini-cli-to-learn-something-new/)
    - ["Creating a Markdown Compiler"](https://thenext.dev/articles/copilot/creating-a-markdown-compiler/) (video)
    - ["Turning a Markdown Spec into a Database - With Test Data!"](https://thenext.dev/articles/video/turning-a-markdown-spec-into-a-database-with-test-data/)

- ["You're Building AI Apps Backwards: The Model-Product Problem"](https://thenewstack.io/youre-building-ai-apps-backwards-the-model-product-problem/)
- ["AI Models Need a Virtual Machine"](https://blog.sigplan.org/2025/08/29/ai-models-need-a-virtual-machine/)

## Criticism

- ["The Copilot Delusion"](https://deplet.ing/the-copilot-delusion/)
- [The Hidden Costs of Coding With Generative AI](https://sloanreview.mit.edu/article/the-hidden-costs-of-coding-with-generative-ai/)
- [The Hidden Costs of AI Coding Assistants: Insights from a Senior Developer](https://blog.devgenius.io/the-hidden-costs-of-ai-coding-assistants-insights-from-a-senior-developer-76274fe6b345)
- ["My new hobby: watching AI slowly drive Microsoft employees insane"](https://www.reddit.com/r/ExperiencedDevs/comments/1krttqo/my_new_hobby_watching_ai_slowly_drive_microsoft/)
- Where's Your Ed At?

    - [Reality Check](https://www.wheresyoured.at/reality-check/)
    - [AI Bubble 2027](https://www.wheresyoured.at/ai-bubble-2027/)
    - [How to Argue with an AI Booster](https://www.wheresyoured.at/how-to-argue-with-an-ai-booster/)
    - [The Haters Guide to The AI Bubble](https://www.wheresyoured.at/the-haters-gui/)

- [We need to stop pretending AI is intelligent -- here's how](https://theconversation.com/we-need-to-stop-pretending-ai-is-intelligent-heres-how-254090)
- [Aritificial intelligence is not intelligent at all](https://www.basicthinking.com/artificial-intelligence-is-not-intelligent-at-all-say-researchers/)
- [AI: Not That Smart](https://columbiacommunityconnection.com/the-dalles/column-ai-not-that-smart)
- [Building AI Products In The Probabilistic Era](https://giansegato.com/essays/probabilistic-era): "Just as physics underwent a conceptual revolution when we moved past Newton's deterministic universe, and into a strange and counterintuitive place made by wave functions, software too is undergoing its own quantum shift. We're leaving a world where code reliably and deterministically takes certain inputs to produce specific outputs, and entering a very different one where machines now produce statistical distributions instead. *Building probabilistic software is like nothing we've done before.*"
- AI Anthropomorphism

    - [Why do Humans Anthropomorphize AI?](https://www.sciencefriday.com/segments/ai-human-personification/)
    - [Are You ...?](https://blog.apaonline.org/2024/08/20/are-you-anthropomorphizing-ai-2/)
    - [We need to stop ...](https://www.reddit.com/r/singularity/comments/1845l4p/we_need_to_stop_anthropomorphizing_ai/)
    - [The Four Degrees of ...](https://www.nngroup.com/articles/anthropomorphism/)
    - [Why Human-like is not human](https://www.copyright.com/blog/anthropomorphizing-ai-why-human-like-is-not-human/)
    - [Anthropomorphism and AI: hype and fallacy](https://link.springer.com/article/10.1007/s43681-024-00419-4) ([PDF](./s43681-024-00419-4.pdf)): "As a form of hype, anthropomorphism is shown to exaggerate AI capabilities and performance by attributing human-like traits to systems that do not possess them. As a fallacy, anthropomorphism is shown to distort moral judgments about AI, such as those concerning its moral character and status, as well as judgments of responsibility and trust. By focusing on these two dimensions of anthropomorphism in AI, the essay highlights negative ethical consequences of the phenomenon in this field."
    - [A Vaccine](https://commoncog.com/vaccine-anthropomorphism-of-ai/): "Here is an explanation that I finally settled on, based on a tweet by prominent AI researcher and educator Andrej Karpathy. When I first tried it out on my dad, he kept quiet for a little bit, and then shifted the way he saw and used LLMs. I’ve tested this on a few friends since, of varying levels of technical sophistication, and am pleased to report that it works quite well. This explanation is useful but not accurate. I’ll give you the explanation, explain why it works, and then give a brief sketch on how it is not true. Finally, I’ll argue that even if it is not accurate, this explanation points you towards more productive mental models of LLMs.
        1. "Imagine that you can visualise words like stars in the sky. The position of the words relative to other words are based on the relationships between each of the words in your language (that is: how closely and how frequently a word appears next to another word in all the sentences ever written). What is important to know is that you can draw arrows from one star to another! These arrows have some surprising properties. One property is that the arrow from the word ‘king’ to the word ‘queen’ is the same as ‘king - boy + girl’. On top of that, let’s imagine that you throw up a starfield for English and a starfield for Spanish. It turns out that if you can draw a one-to-one mapping between the two starfields, the king to queen arrow is the same in both languages! This was a very surprising finding when it first came out!
        2. " What a Large Language Model is is that it is a very sophisticated auto complete. But it is a bit more than that.
        3. "When you ask “what are the 10 best places to visit in Bali” the AI will give you a plausible-sounding answer. But the way it gives you that answer is that during the AI’s training, some human somewhere wrote an answer like “the top 10 places to visit in London” and “the top 10 places to visit in Tokyo” and “the top 10 places to visit in New York” based on some cursory research and Google searches. Then the AI took those written examples, and memorised the statistical relationships between the sentences, which you can imagine like the arrows between large clusters of stars. Then when you ask “what are the 10 best places in Bali or Singapore or Lisbon”, it just moves the arrows it learnt over to the part of its starfield that has concepts related to Bali, Singapore and Lisbon, and spits out something similar to what the human trainer wrote.
        4. "Notice that this is not thinking. The LLM is doing autocomplete, but using this ‘arrow in the starfield’ property to give you very good, plausible-sounding, novel answers.
        5. "But because it writes so eloquently, and answers you like a human would, you think that it’s actually intelligent and sentient. I suppose you could say that it is ‘intelligent’ (by some definition of ‘intelligent’) but it is not a person. It doesn’t understand concepts the way a human does. What it is relying on is some complex version of this ‘arrow in a starfield’ property.
        6. "You may get better answers by constraining it to a smaller set of documents. So if you give it a bunch of papers, and ask it for themes or a summary of those papers, it may give you better answers than if you assumed those papers were in its original training corpus.

        "And that’s it."

- [OpenAI Realizes It Made a Terrible Mistake](https://futurism.com/openai-mistake-hallucinations): "In a [paper](https://arxiv.org/abs/2509.04664) published last week, a team of OpenAI researchers attempted to come up with an explanation. They suggest that large language models hallucinate because when they're being created, they're incentivized to guess rather than admit they simply don't know the answer. ... In simple terms, in other words, guessing is rewarded — because it might be right — over an AI admitting it doesn't know the answer, which will be graded as incorrect no matter what. As a result, through "natural statistical pressures," LLMs are far more prone to hallucinate an answer instead of "acknowledging uncertainty.""
- [Hallucinations getting worse as AI models get more capable](https://futurism.com/ai-industry-problem-smarter-hallucinating)

## Tools

* [Lovable](https://lovable.dev/): Create apps and websites by chatting with AI
* ChatGPT-5

    - https://futurism.com/gpt-5-disaster
    - [ChatGPT Is Blowing Up Marriages as Spouses Use AI to Attack Their Partners](https://futurism.com/chatgpt-marriages-divorces)

## Tutorials

- https://towardsdatascience.com/langgraph-101-lets-build-a-deep-research-agent/ : Learn LangGraph fundamentals from Google’s open-source full-stack implementation

## Reading

* [Himanshu](https://www.linkedin.com/posts/himanshu1707_generativeai-journey-through-paperbacks-activity-7357857323756761088-elA4/):

    1. *Natural Language Processing with Transformers*: Start with a thorough understanding of Transformers architecture and it's ecosystem through HuggingFace with Lewis Tunstall Leandro von Werra Thomas Wolf . Personally, There cannot be a better starting point. MUST follow their blogs.
    2. *Build a Large Language Model (From Scratch)*: Build your own Decoder Transformer(Architecture for "almost" all modern LLMs) from scratch with Sebastian Raschka, PhD . MUST follow his blogs too for the latest updates. Also, lucky he keeps his GitHub repo updated.
    3. Get acquainted with the GenerativeAI ecosystem with Jay Alammar Maarten Grootendorst building LLM pipelines for various tasks and learning nitty gritty details of Fine-Tuning and PEFT. *Hands-On Language Models*
    4. Explore *Generative AI on AWS* with Chris Fregly Antje Barth Shelbee Eigenbrode for an enterprise level challenges.
    5. Dive into the beauty of engineering a GenAI Application with *AI Engineering* by Chip Huyen . It will level you up to think from the whole system perspective.
    6. *Build Generative AI Services with FastAPI* with Ali Parandeh, CEng . A very practical book to knot all you have learnt and will help you in building a GenAI system.

* [LangGraph101: Let's build a deep research agent](https://towardsdatascience.com/langgraph-101-lets-build-a-deep-research-agent/)

## Definitions

### [Expert Systems](/tags/expert%20system.html)/[Rules Engines](/tags/rules.html)

### Fuzzy Logic

Java:

- https://jfuzzylogic.sourceforge.net/
- https://github.com/sorend/fuzzy4j
- https://github.com/pcingola/jFuzzyLogic
- https://commons.apache.org/sandbox/commons-text/jacoco/org.apache.commons.text.similarity/FuzzyScore.java.html
- https://mvnrepository.com/artifact/com.github.cschen1205/java-fuzzy-logic/1.0.1

### Natural Language Processing

* [You Don't Need Backpropagation To Train Neural Networks Anymore](https://ai.gopubby.com/you-dont-need-backpropagation-to-train-neural-networks-anymore-e989d75564cb)

### Natural Language Programming

* [Extract, Edit, Apply](https://githubnext.com/projects/extract-edit-apply/)
* [A design pattern for AI](https://dsyme.net/2025/02/12/extract-edit-apply/)
* [On natural language programming](https://dsyme.net/2025/08/27/on-natural-language-programming/)
* [Why Kind of Programming is Natural Language Programming?](https://dsyme.net/2025/09/02/what-kind-of-programming-is-natural-language-programming/)

### [Large Language Models (LLMs)](/ai/llm/)

### [Small Language Models (SLMs)](/ai/slm/)

### [Retrieval Augmented Generation (RAG)](/ai/rag)

### Coding Assistants

* [Create a Coding Assistant with StarCoder](https://huggingface.co/blog/starchat-alpha)

### Generative AI

* https://www.kdnuggets.com/generative-ai-a-self-study-roadmap
* ["Godot isn't making it"](https://www.wheresyoured.at/godot-isnt-making-it/): "What if what we're seeing today isn't a glimpse of the future, but the new terms of the present? What if artificial intelligence isn't actually capable of doing much more than what we're seeing today, and what if there's no clear timeline when it'll be able to do more? What if this entire hype cycle has been built, goosed by a compliant media ready and willing to take career-embellishers at their word? (March 2024) The reason I'm repeating myself is that it's important to note how obvious the problems with generative AI have been, and for how long."
* [Generative AI exists because of the transformer](https://ig.ft.com/generative-ai/)
* ["Stable Diffusion in Java (SD4J) Enables Generating Images with Deep Learning"](https://www.infoq.com/news/2023/12/stable-diffusion-in-java/)
* [Stable Diffusion in C#](https://github.com/cassiebreviu/StableDiffusion/) (mentioned in the SD4J article)

### Machine Learning

* [10 Github Repositories to Master Reinforcement Learning](https://www.kdnuggets.com/10-github-repositories-master-reinforcement-learning)
* [Machine Learning for Software Engineering](https://github.com/saltudelft/ml4se): A curated list of papers, theses, datasets, and tools related to the application of Machine Learning for Software Engineering.
* [Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and Mitigations](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2023.pdf)

### Semantic Entity Resolution (Knowledge Graphs) (?)

* [The Rise of Semantic Entity Resolution](https://towardsdatascience.com/the-rise-of-semantic-entity-resolution/)

