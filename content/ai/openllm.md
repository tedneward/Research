title=OpenLLM
tags=ai, runtime
summary=Run any open-source LLMs, such as DeepSeek and Llama, as OpenAI compatible API endpoint in the cloud.
~~~~~~

[Website](https://bentoml.com/) | [Source](https://github.com/bentoml/OpenLLM)

OpenLLM allows developers to run **any open-source LLMs** (Llama 3.3, Qwen2.5, Phi3 and [more](https://github.com/bentoml/OpenLLM#supported-models)) or **custom models** as **OpenAI-compatible APIs** with a single command. It features a [built-in chat UI](https://github.com/bentoml/OpenLLM#chat-ui), state-of-the-art inference backends, and a simplified workflow for creating enterprise-grade cloud deployment with Docker, Kubernetes, and [BentoCloud](https://github.com/bentoml/OpenLLM#deploy-to-bentocloud).

Understand the [design philosophy of OpenLLM](https://www.bentoml.com/blog/from-ollama-to-openllm-running-llms-in-the-cloud).

